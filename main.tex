\documentclass[10pt]{article}
\usepackage{color}
\usepackage{xcolor}
\usepackage{url}
\usepackage{ulem}
\usepackage{mdwlist}
\usepackage{booktabs}
\usepackage{my-stdpkgs}
\usepackage{my-typography}
\usepackage{caption}
%\usepackage[numbers,sort&compress]{natbib}


%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{morefloats}
\setcounter{page}{1}
\pagenumbering{arabic}

\makeatletter
\newcommand{\ccell}[3][]{%
  \kern-\fboxsep
  \if\relax\detokenize{#1}\relax
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\colorbox{#2}}%
  {\colorbox[#1]{#2}}%
  {#3}\kern-\fboxsep
}
\makeatother
\definecolor{cellgray}{gray}{0.9}

\newcommand{\yesy}{\ccell{green}{Y}}
\newcommand{\non}{\ccell{red}{N}}
\newcommand{\myhline}{}


%TODO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Discuss title
%Do we discuss dense vs. sparse for each case or have a separate section for sparse?
%Should we consider graphical models?
%Should abstract emphasize bringing TD's to CS community, or emphasize computational limits?
%Move to bitbucket
%By next meeting make a grid of software :  latex booktabs



%Tentative title
\title{Data Analysis using Tensor Decompositions: Challenges and Applications (TODO)}

%Tentative author list
\author{
  Casey Battaglino$^\ast$, Joachim Perros$^\ast$, Jiajia Li$^\ast$, \\
   Jimeng Sun$^\ast$, Richard Vuduc$^\ast$
  \\ Georgia Institute of Technology, Atlanta, GA
  \\ $^\ast$ School of Computational Science and Engineering \\
  \{cbattaglino3,perros,jiajiali,jsun,richie\}@gatech.edu } \date{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

\begin{abstract}
Linear algebra underpins the majority of data analysis techniques, but multi-linear algebra remains a fringe topic despite its natural applicability to the high-dimensionality that arises in a Big Data setting. Tensor methods have been the subject of much research in signal processing, physics, and applied mathematics, but haven't seen wide adoption in the wider computer science community. One particularly difficult and relevant issue is the efficient low-rank representation of highly sparse, high-dimensional data.

This survey has two goals: to demistify tensor approaches in data science (by moving away from clunky traditional notation), and to clearly delineate where current approaches (both dense and sparse) become computationally infeasible as a call towards future research in scalability.


\end{abstract}

\tableofcontents
%=======================================================================
\section{Introduction} \label{sec:intro}

%=======================================================================
\section{Background} \label{sec:background}
\subsection{Definitions} \label{sec:definitions}
\subsubsection{Tensor Representation}
\subsubsection{Multilinear Rank}
Computing rank is NP complete, and best-rank approximation is ill-posed~\cite{Kolda09tensordecompositions}
\subsubsection{Tensor Algebra}
\subsubsection{Tensor Contractions}
\subsubsection{Tensor Network Diagrams}

\section{Data Science Applications}
In signal processing we may have both temporal and spatial components to our signal (in combination with frequency)

Importance of multi-linear rank-reduction.

Feature selection, feature extraction, classification, multi-way clustering.
Blind-source separation. 
psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, watermarking, numerical analysis, data mining, neuroscience, graph analysis, ICA.
quantum chemistry(Hartree-Fock, MD), EM wave scattering/logging. 3-dimensional PDEs. 

Anandkumar papers~\cite{Anandk}
%=======================================================================
\section{Low-Rank Approximations}
%mention need for compression

Tucker, CP are old concepts. Decomposition into block terms~\cite{Lathauwer08decompositionsof} generalizes standard methods.

Neat application: Bblinear maps can be generalized as tensors, so tensor decompositions can yield faster algorithms for matrix multiplication, i.e~\cite{Benson}

\subsection{CP Decomposition}
Introduced:~\cite{hitchcock-sum-1927}
CP breaks curse of dimensionality: instead of $n^d$ parameters we need only $dnr$.
CP is ill-posed, different regularizations can be used.
CP assumes every tensor mode is involved with every `concept.' Restrictive for noisy higher order data (-Joachim)
\subsection{Tucker Decomposition}
Introduced:~\cite{Tuck1966c}
Instead of $n^d$ parameters we need only $\prod_{i=1}^d r_i + n\sum_{i=1}^d r_i$.
Stable, we can use SVD~\cite{Lathauwer00amultilinear}.
We can determine $r_1, \dots, r_d$ as a function of $\epsilon$.

If the Tucker Decomposition is properly normalized, it is also considered an HOSVD.

Exponential storage, time. In sparse case, we have same problem: dense results (-Joachim)
\subsection{Hierarchical Tucker}
Introduced:~\cite{Grasedyck:2010:HSV:1958286.1958311}
B's can express a linear combination of vectors in the U's below them.
Obst 1: SVD of tensor unfoldings
Obst 2: Intermediate memory blowup (computing core tensor)... dense intermediate result
\subsubsection{Sparse H-Tucker}
If we use CUR decomposition instead of SVD and store only those columns and rows that contain an element, we can greatly reduce storage and retain sparsity of 

\subsection{Tensor Train}
Introduced:~\cite{Oseledets:2011:TD:2079141.2079149}
%=======================================================================
\section{Computational Challenges}
\subsection{NP-Hardness}
Most tensor problems are NP-Hard~\cite{Hillar}.
\subsection{Curse of Dimensionality}
\subsubsection{Time}
\subsubsection{Memory Blowup}
\subsubsection{Storage}
\subsection{Memory Efficiency}
\subsubsection{Representation Issues}
\subsubsection{Distributed Memory Issues}
\subsubsection{Communication Bounds}
\subsection{Computational Scalability}
There is a discussion of recent developments in distributed scalability of tensor algorithms~\cite{falbull}.
\subsection{Sparsity}
\subsection{Stability}
%=======================================================================
%Section on: common operations? computational bottlenecks?
\section{Experiments}
\subsection{Breaking Points}
\subsubsection{CP Decomposition}
\subsubsection{Tucker/HOSVD}
\subsubsection{Hierarchical Tucker}
\subsubsection{Tensor Train}
\subsection{Comparisons}
%=======================================================================
\section{Implementations}
\include{software}
%=======================================================================
\section{Conclusions} \label{sec:conclusions}
%=======================================================================


%=======================================================================
%\section{Acknowledgments} \label{sec:acknowledgments}
%=======================================================================

\bibliographystyle{abbrv} \bibliography{tensbib}

\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%eof
