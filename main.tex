\documentclass[10pt]{article}
\usepackage{color}
\usepackage{xcolor}
\usepackage{url}
\usepackage{ulem}
\usepackage{mdwlist}
\usepackage{booktabs}
\usepackage{my-stdpkgs}
\usepackage{my-typography}
\usepackage{caption}
%\usepackage[numbers,sort&compress]{natbib}


%\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{float}
\usepackage{amsfonts}
\usepackage{morefloats}
\setcounter{page}{1}
\pagenumbering{arabic}

\makeatletter
\newcommand{\ccell}[3][]{%
  \kern-\fboxsep
  \if\relax\detokenize{#1}\relax
    \expandafter\@firstoftwo
  \else
    \expandafter\@secondoftwo
  \fi
  {\colorbox{#2}}%
  {\colorbox[#1]{#2}}%
  {#3}\kern-\fboxsep
}
\makeatother
\definecolor{cellgray}{gray}{0.9}

\newcommand{\yesy}{\ccell{green}{Y}}
\newcommand{\non}{\ccell{red}{N}}
\newcommand{\myhline}{}


%TODO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Discuss title
%Do we discuss dense vs. sparse for each case or have a separate section for sparse?
%Should we consider graphical models?
%Should abstract emphasize bringing TD's to CS community, or emphasize computational limits?
%Move to bitbucket
%By next meeting make a grid of software :  latex booktabs



%Tentative title
\title{Data Analysis using Tensor Decompositions: Challenges and Applications (TODO)}

%Tentative author list
\author{
  Casey Battaglino$^\ast$, Joachim Perros$^\ast$, Jiajia Li$^\ast$, \\
   Jimeng Sun$^\ast$, Richard Vuduc$^\ast$
  \\ Georgia Institute of Technology, Atlanta, GA
  \\ $^\ast$ School of Computational Science and Engineering \\
  \{cbattaglino3,perros,jiajiali,jsun,richie\}@gatech.edu } \date{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

\begin{abstract}
Linear algebra underpins the majority of data analysis techniques, but multi-linear algebra remains a fringe topic despite its natural applicability to the high-dimensionality that arises in a Big Data setting. Tensor methods have been the subject of much research in signal processing, physics, and applied mathematics, but haven't seen wide adoption in the wider computer science community. One particularly difficult and relevant issue is the efficient low-rank representation of highly sparse, high-dimensional data.

This survey has two goals: to demistify tensor approaches in data science (by moving away from clunky traditional notation), and to clearly delineate where current approaches (both dense and sparse) become computationally infeasible as a call towards future research in scalability.


\end{abstract}

\tableofcontents
%=======================================================================
\section{Introduction} \label{sec:intro}

%=======================================================================
\section{Background} \label{sec:background}
\subsection{Definitions} \label{sec:definitions}
\subsubsection{Tensor Representation}
\subsubsection{Multilinear Rank}
Computing rank is NP complete, and best-rank approximation is ill-posed~\cite{Kolda09tensordecompositions}
\subsubsection{Tensor Algebra}
\subsubsection{Tensor Contractions}
\subsubsection{Tensor Network Diagrams}

\section{Data Science Applications}
In signal processing we may have both temporal and spatial components to our signal (in combination with frequency)

Importance of multi-linear rank-reduction.

Feature selection, feature extraction, classification, multi-way clustering.
Blind-source separation. 
psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, watermarking, numerical analysis, data mining, neuroscience, graph analysis, ICA.
quantum chemistry(Hartree-Fock, MD), EM wave scattering/logging. 3-dimensional PDEs. 
%=======================================================================
\section{Low-Rank Approximations}
%mention need for compression

Tucker, CP are old concepts. Decomposition into block terms~\cite{Lathauwer08decompositionsof} generalizes standard methods.

\subsection{CP Decomposition}
Introduced:~\cite{hitchcock-sum-1927}
CP breaks curse of dimensionality: instead of $n^d$ parameters we need only $dnr$.
CP is ill-posed, different regularizations can be used.
CP assumes every tensor mode is involved with every `concept.' Restrictive for noisy higher order data (-Joachim)
\subsection{Tucker Decomposition}
Introduced:~\cite{Tuck1966c}
Instead of $n^d$ parameters we need only $\prod_{i=1}^d r_i + n\sum_{i=1}^d r_i$.
Stable, we can use SVD~\cite{Lathauwer00amultilinear}.
We can determine $r_1, \dots, r_d$ as a function of $\epsilon$.

If the Tucker Decomposition is properly normalized, it is also considered an HOSVD.

Exponential storage, time. In sparse case, we have same problem: dense results (-Joachim)
\subsection{Hierarchical Tucker}
Introduced:~\cite{Grasedyck:2010:HSV:1958286.1958311}
B's can express a linear combination of vectors in the U's below them.
Obst 1: SVD of tensor unfoldings
Obst 2: Intermediate memory blowup (computing core tensor)... dense intermediate result
\subsubsection{Sparse H-Tucker}
\subsection{Tensor Train}
Introduced:~\cite{Oseledets:2011:TD:2079141.2079149}
%=======================================================================
\section{Computational Challenges}
\subsection{Curse of Dimensionality}
\subsubsection{Time}
\subsubsection{Memory Blowup}
\subsubsection{Storage}
\subsection{Memory Efficiency}
\subsubsection{Representation Issues}
\subsubsection{Distributed Memory Issues}
\subsubsection{Communication Bounds}
\subsection{Computational Scalability}
There is a discussion of recent developments in distributed scalability of tensor algorithms~\cite{falbull}.
\subsection{Sparsity}
\subsection{Stability}
%=======================================================================
%Section on: common operations? computational bottlenecks?
\section{Experiments}
\subsection{Breaking Points}
\subsubsection{CP Decomposition}
\subsubsection{Tucker/HOSVD}
\subsubsection{Hierarchical Tucker}
\subsubsection{Tensor Train}
\subsection{Comparisons}
%=======================================================================
\section{Implementations}

The landscape of tensor libraries is growing quickly, but the capabilities of each implementation varies widely and often focuses on a small subset of mathematical methods, often in support of a field-specific application. Native support of tensor data structures is still lacking in nearly every widely-adopted programming language, although powerful open-source libraries are available to the C++ and MATLAB communities. 

While software landscape is expected to change greatly in the near future, this section aims to document the capabilities of current implementations at  publication date.

\subsection{Data Structures}
Recent work has introduced more efficient data structures for distributed dense and sparsetensors.

Discuss Baskaran et. al's representation~\cite{6408676}.

Discuss Cyclops approach~\cite{CTF}.

\subsection{Basic Arithmetic}
(Addition, multiplication)

Discuss indexing approach.

FTensor, LTensor, Blitz++, HUJI

\subsection{Matricization and Tensorization}

\subsection{Tensor Multiplication}

\subsection{Tensor Contraction}
Discuss optimization approach taken by Tensor Contraction Engine~\cite{TCE}.

\subsection{Tensor Decompositions}
Discuss how a variety of different algorithms can be taken to compute the same decomposition, (i.e ALS, CG, etc) with examples.

Discuss GigaTensor, DFacto.

\subsection{Tensor Networks}

\begin{center}\scriptsize
    \begin{tabular}{ |l  l  c  c  c  c  c  l|}
    \toprule
    \textbf{Implementation} & \textbf{Environment} & $\underline{A} \times \underline{B}$ & $\underline{A} \otimes \underline{B}$ & \textbf{CP} & \textbf{Tucker} & \textbf{TN} & \textbf{Domain} \\ \hline
    %scikit-tensor					
    %& Python & \yesy & \yesy & \yesy & \yesy & \non & Phys. \\ \myhline
    PyTensor~\cite{Yoo10pytensor:a}				
    & Python & \yesy & \non & \non & \yesy & \non & Gen. \\ \myhline
    tensor (Theano)             
    & Python & \yesy & \yesy & \non & \non & \non & ML \\ \myhline
    SPLATT~\cite{}
    & C & \yesy & \non & \non & \non & \non & Gen. \\ \myhline
    FTensor~\cite{Landry:2003:IHP:1240120.1240122,FTensor}					
    & C++ & \yesy & \yesy & \non & \non & \non & Phys. \\ \myhline
    LTensor~\cite{LTensor} 		
    & C++ & \yesy & \yesy & \non & \non & \non & Phys. \\ \myhline
    Blitz++~\cite{blitz} 
    & C++ & \yesy & \yesy & \non & \non & \non & Gen. \\ \myhline
    Boost MDA~\cite{boost-multiarray} 
    & C++ & \non & \non & \non & \non & \non & Gen. \\ \myhline
    C++ Tensor TB			
    & C++ & \yesy & \yesy & \yesy & \non & \non & Gen. \\ \myhline
    HUJI~\cite{huji} 		
    & C++ & \non & \non & \non & \non & \non & Gen. \\ \myhline
    Cyclops~\cite{CTF}
    & C++ & \yesy & \yesy & \non & \non & \non & Gen. \\ \myhline
    TensorCalculus~\cite{Calculus}
    & C++ & \yesy & \yesy & \yesy & \yesy & \yesy & Gen. \\ \myhline
    TNT Library~\cite{TNT}
    & C++ & \non ? & \yesy & \non & \non & \yesy & Phys. \\ \myhline
    Tensor Toolbox~\cite{TensorToolbox}
    & MATLAB & \yesy & \yesy & \yesy & \yesy & \non & Gen. \\ \myhline
    N-way Toolbox~\cite{Nway-Paper,Nway}
    & MATLAB & \yesy & \yesy & \yesy & \yesy & \non & Chem. \\ \myhline
    CuBatch~\cite{CuBatch}
    & MATLAB & \yesy & \yesy & \yesy & \yesy & \non & ML \\ \myhline
    PLS-Toolbox~\cite{PLS-toolbox}
    & MATLAB & \yesy & \yesy & \non & \yesy & \non & Chem. \\ \myhline
    TDALAB~\cite{TDALAB,TDALAB_online}
    & MATLAB & \yesy & \yesy & \yesy & \yesy & \non & Gen. \\ \myhline
    TENSORBOX~\cite{TENSORBOX}
    & MATLAB & \yesy & \yesy & \yesy & \yesy & \non & Gen. \\ \myhline
    Tensorlab~\cite{Tensorlab}
    & MATLAB & \yesy & \yesy & \yesy & \non & \non & Gen. \\ \myhline
    \textsc{ParCube}~\cite{PARCUBE}
    & MATLAB & \yesy & \yesy & \yesy & \non & \non & Gen. \\ \myhline
    TT-Toolbox~\cite{tt-toolbox}
    & MATLAB & \yesy & \yesy & \non & \non & \ccell{green}{TT} & Gen. \\ \myhline
    htucker~\cite{HT,Kressner:2014:A9H:2610268.2538688} & MATLAB & \yesy & \yesy & \non & \non & \ccell{green}{HT} & General \\ \myhline
    TCE~\cite{TCE}
    & DSL & \yesy & \yesy & \non & \non & \non & Chem. \\ \hline
    \end{tabular}
    \captionof{table}{Software support for native tensor arithmetic and decompositions.} \label{tab:typesupport}
\end{center}

\begin{center}\scriptsize
    \begin{tabular}{ |l  c  c  c|}
    \toprule
    \textbf{Implementation} & \textbf{Dense} & \textbf{Sparse} & \textbf{Distributed}  \\ \hline
    %scikit-tensor                   
    %& \yesy & \non & \non \\ \myhline
    PyTensor~\cite{Yoo10pytensor:a}             
    & \yesy & \yesy & \non \\ \myhline
    tensor (Theano)             
    & \yesy & \non & \non \\ \myhline
    SPLATT~\cite{}
    & \non & \yesy & \yesy \\ \myhline
    FTensor~\cite{Landry:2003:IHP:1240120.1240122,FTensor}                  
    & \yesy & \non & \non \\ \myhline
    LTensor~\cite{LTensor}      
    & \yesy & \non & \non \\ \myhline
    Blitz++~\cite{blitz} 
    & \yesy & \yesy ? & \non \\ \myhline
    Boost MDA~\cite{boost-multiarray} 
    & \yesy & \non & \non \\ \myhline
    C++ Tensor TB             
    & \yesy & \yesy & \non \\ \myhline
    HUJI~\cite{huji}         
    & \yesy & \yesy & \non \\ \myhline
    Cyclops~\cite{CTF}
    & \yesy & \non & \ccell{green}{MPI} \\ \myhline
    TensorCalculus~\cite{Calculus}
    & \yesy & \non & \non ? \\ \myhline
    TNT Library~\cite{TNT}
    & \yesy & \yesy & \non \\ \myhline
    Tensor Toolbox~\cite{TensorToolbox}
    & \yesy & \yesy & \non \\ \myhline
    N-way Toolbox~\cite{Nway-Paper,Nway}
    & \yesy & \yesy & \non \\ \myhline
    CuBatch~\cite{CuBatch}
    & \yesy & \yesy & \non \\ \myhline
    PLS-Toolbox~\cite{PLS-toolbox}
    & \yesy & \yesy & \non \\ \myhline
    TDALAB~\cite{TDALAB,TDALAB_online}
    & \yesy & \yesy & \non \\ \myhline
    TENSORBOX~\cite{TENSORBOX}
    & \yesy & \yesy & \non \\ \myhline
    Tensorlab~\cite{Tensorlab}
    & \yesy & \yesy & \non \\ \myhline
    \textsc{ParCube}~\cite{PARCUBE}
    & \yesy & \yesy & \yesy \\ \myhline
    TT-Toolbox~\cite{tt-toolbox}
    & \yesy & \non & \non \\ \myhline
    htucker~\cite{HT,Kressner:2014:A9H:2610268.2538688} & 
    \yesy & \yesy? & \non \\ \myhline
    TCE~\cite{TCE}
    & \yesy & \non ? & \yesy \\ \hline
    \end{tabular}
    \captionof{table}{Software support for native dense, sparse, and distributed tensors.} \label{tab:typesupport}
\end{center}
%=======================================================================
\section{Conclusions} \label{sec:conclusions}
%=======================================================================


%=======================================================================
%\section{Acknowledgments} \label{sec:acknowledgments}
%=======================================================================

\bibliographystyle{abbrv} \bibliography{tensbib}

\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%eof
