\documentclass[10pt]{article}
\usepackage{color}
\usepackage{url}
\usepackage{ulem}
\usepackage{mdwlist}
\usepackage[numbers,sort&compress]{natbib}
\setcounter{page}{1}
\pagenumbering{arabic}

%TODO
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Discuss title
%Do we discuss dense vs. sparse for each case or have a separate section for sparse?
%Should we consider graphical models?
%Should abstract emphasize bringing TD's to CS community, or emphasize computational limits?


%Tentative title
\title{Data Analysis with Tensor Decompositions}

%Tentative author list
\author{
  Casey Battaglino$^\ast$, Joachim Perros$^\ast$, Jiajia Li$^\ast$, \\
   Jimeng Sun$^\ast$, Richard Vuduc$^\ast$
  \\ Georgia Institute of Technology, Atlanta, GA
  \\ $^\ast$ School of Computational Science and Engineering \\
  \{cbattaglino3,perros,jiajiali,jsun,richie\}@gatech.edu } \date{}

\newcommand{\Eye}[1]{{\emph{#1}}}
\newcommand{\TODO}[1]{{\color{red}\textbf{#1}}}
\newcommand{\IGNORE}[1]{}
\newcommand{\bigO}[1]{\ensuremath{\mathcal{O}\left(#1\right)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

\begin{abstract}
Linear algebra underpins the majority of data analysis techniques, but multi-linear algebra remains a fringe topic despite its natural applicability to the high-dimensionality that arises in a Big Data setting. Tensor methods have been the subject of much research in signal processing, physics, and applied mathematics, but haven't seen wide adoption in the wider computer science community.

This survey has two goals: to demistify tensor approaches in data science (by moving away from clunky traditional notation), and to clearly delineate where current approaches become computationally infeasible as a call towards future research in scalability.


\end{abstract}

\tableofcontents
%=======================================================================
\section{Introduction} \label{sec:intro}

%=======================================================================
\section{Background} \label{sec:background}
\subsection{Definitions} \label{sec:definitions}
\subsubsection{Tensor Representation}
\subsubsection{Multilinear Rank}
Computing rank is NP complete, and best-rank approximation is ill-posed~\cite{Kolda09tensordecompositions}
\subsubsection{Tensor Algebra}
\subsubsection{Tensor Contractions}
\subsubsection{Tensor Network Diagrams}
\subsection{Applications}
In signal processing we may have both temporal and spatial components to our signal (in combination with frequency)

Importance of multi-linear rank-reduction.

Feature selection, feature extraction, classification, multi-way clustering.
Blind-source separation. 
psycho-metrics, chemometrics, signal processing, numerical linear algebra, computer vision, watermarking, numerical analysis, data mining, neuroscience, graph analysis, ICA.
quantum chemistry(Hartree-Fock, MD), EM wave scattering/logging. 3-dimensional PDEs. 
%=======================================================================
\section{Low-Rank Approximations}
%mention need for compression

Tucker, CP are old concepts. Decomposition into block terms~\cite{Lathauwer08decompositionsof} generalizes standard methods.

\subsection{CP Decomposition}
Introduced:~\cite{hitchcock-sum-1927}
CP breaks curse of dimensionality: instead of $n^d$ parameters we need only $dnr$.
CP is ill-posed, different regularizations can be used.
\subsection{Tucker Decomposition}
Introduced:~\cite{Tuck1966c}
Instead of $n^d$ parameters we need only $\prod_{i=1}^d r_i + n\sum_{i=1}^d r_i$.
Stable, we can use SVD~\cite{Lathauwer00amultilinear}.
We can determine $r_1, \dots, r_d$ as a function of $\epsilon$.

If the Tucker Decomposition is properly normalized, it is also considered an HOSVD.
\subsection{Hierarchical Tucker}
Introduced:~\cite{Grasedyck:2010:HSV:1958286.1958311}
\subsection{Tensor Train}
Introduced:~\cite{Oseledets:2011:TD:2079141.2079149}
%=======================================================================
\section{Computational Challenges}
\subsection{Curse of Dimensionality}
\subsubsection{Time}
\subsubsection{Memory Blowup}
\subsubsection{Storage}
\subsection{Memory Efficiency}
\subsubsection{Representation Issues}
\subsubsection{Distributed Memory Issues}
\subsubsection{Communication Bounds}
\subsection{Sparsity}
\subsection{Stability}
%=======================================================================
%Section on: common operations? computational bottlenecks?
\section{Experiments}
\subsection{Breaking Points}
\subsubsection{CP Decomposition}
\subsubsection{Tucker/HOSVD}
\subsubsection{Hierarchical Tucker}
\subsubsection{Tensor Train}
\subsection{Comparisons}
%=======================================================================
\section{Current Implementations}
\subsection{Tensor Toolbox}


%=======================================================================
\section{Conclusions} \label{sec:conclusions}
%=======================================================================


%=======================================================================
%\section{Acknowledgments} \label{sec:acknowledgments}
%=======================================================================

\bibliographystyle{abbrv} \bibliography{tensbib}

\input{appendix}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%eof
